{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b942d471",
   "metadata": {},
   "source": [
    "## Large-Scale External Dataset Evaluation\n",
    "**(This notebook was ran on colab because of compute reasons, .pkl file was exported which is shown at the end of notebook)**\n",
    "\n",
    "**Testing the Human vs AI Classifier on External Data**\n",
    "\n",
    "This notebook tests the fine-tuned DistilBERT+LoRA model on a completely independent dataset from Hugging Face to evaluate generalization beyond the original training distribution.\n",
    "\n",
    "**Dataset**: `gsingh1-py/train` (first 3000 samples)\n",
    "- Contains human-written text and Mistral-generated text\n",
    "- Completely independent from training data\n",
    "- Tests whether model learned genuine AI detection or dataset artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b1322",
   "metadata": {},
   "source": [
    "## Task 1: Load and Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3286a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset from Hugging Face...\")\n",
    "ds = load_dataset(\"gsingh1-py/train\")\n",
    "\n",
    "print(f\"\\nDataset structure: {ds}\")\n",
    "print(f\"\\nDataset splits: {list(ds.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd225ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the fields/columns in the dataset\n",
    "if 'train' in ds:\n",
    "    sample_data = ds['train']\n",
    "else:\n",
    "    # If there's no 'train' split, use the first available split\n",
    "    split_name = list(ds.keys())[0]\n",
    "    sample_data = ds[split_name]\n",
    "    print(f\"Using split: {split_name}\")\n",
    "\n",
    "print(f\"\\nDataset features/columns: {sample_data.features}\")\n",
    "print(f\"\\nTotal rows in dataset: {len(sample_data)}\")\n",
    "\n",
    "# Display first few examples to understand the structure\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE EXAMPLES FROM DATASET:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(min(3, len(sample_data))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    example = sample_data[i]\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 200:\n",
    "            print(f\"{key}: {value[:200]}... (truncated, total length: {len(value)})\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the fields we need: human text and Mistral-generated text\n",
    "# We'll need to identify the correct field names first\n",
    "\n",
    "print(\"\\nIdentifying relevant fields...\")\n",
    "feature_names = list(sample_data.features.keys())\n",
    "print(f\"Available fields: {feature_names}\")\n",
    "\n",
    "# Look for fields containing 'human', 'mistral', 'generated', 'text', etc.\n",
    "human_field = None\n",
    "mistral_field = None\n",
    "\n",
    "for field in feature_names:\n",
    "    field_lower = field.lower()\n",
    "    if 'human' in field_lower and 'mistral' not in field_lower:\n",
    "        human_field = field\n",
    "        print(f\"Identified human text field: {field}\")\n",
    "    if 'mistral' in field_lower:\n",
    "        mistral_field = field\n",
    "        print(f\"Identified Mistral text field: {field}\")\n",
    "\n",
    "# If not found by name, let's examine the data structure more carefully\n",
    "if human_field is None or mistral_field is None:\n",
    "    print(\"\\nCould not auto-identify fields. Examining data structure...\")\n",
    "    example = sample_data[0]\n",
    "    print(\"\\nField name -> Sample content:\")\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these field names after running the cell above\n",
    "HUMAN_FIELD = 'human_text'  # Replace with actual field name\n",
    "MISTRAL_FIELD = 'mistral_text'  # Replace with actual field name\n",
    "\n",
    "print(f\"Using fields:\")\n",
    "print(f\"  Human: {HUMAN_FIELD}\")\n",
    "print(f\"  Mistral: {MISTRAL_FIELD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 3000\n",
    "eval_texts = []\n",
    "eval_labels = []\n",
    "eval_sources = []  # Track which class each sample belongs to\n",
    "\n",
    "print(f\"\\nExtracting first {N_SAMPLES} samples...\")\n",
    "\n",
    "# Extract human texts\n",
    "for i in range(min(N_SAMPLES, len(sample_data))):\n",
    "    human_text = sample_data[i][HUMAN_FIELD]\n",
    "    mistral_text = sample_data[i][MISTRAL_FIELD]\n",
    "    \n",
    "    # Add human text\n",
    "    if human_text and isinstance(human_text, str) and len(human_text.strip()) > 0:\n",
    "        eval_texts.append(human_text.strip())\n",
    "        eval_labels.append(0)  # 0 = Human\n",
    "        eval_sources.append('Human')\n",
    "    \n",
    "    # Add Mistral text\n",
    "    if mistral_text and isinstance(mistral_text, str) and len(mistral_text.strip()) > 0:\n",
    "        eval_texts.append(mistral_text.strip())\n",
    "        eval_labels.append(1)  # 1 = AI/Mistral\n",
    "        eval_sources.append('Mistral')\n",
    "\n",
    "print(f\"\\nTotal samples extracted: {len(eval_texts)}\")\n",
    "print(f\"  Human: {eval_labels.count(0)}\")\n",
    "print(f\"  Mistral (AI): {eval_labels.count(1)}\")\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "eval_df = pd.DataFrame({\n",
    "    'text': eval_texts,\n",
    "    'true_label': eval_labels,\n",
    "    'source': eval_sources\n",
    "})\n",
    "\n",
    "print(f\"\\nDataFrame shape: {eval_df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(eval_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde84d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample texts\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE HUMAN TEXT:\")\n",
    "print(\"=\"*80)\n",
    "print(eval_df[eval_df['source'] == 'Human']['text'].iloc[0][:500])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE MISTRAL TEXT:\")\n",
    "print(\"=\"*80)\n",
    "print(eval_df[eval_df['source'] == 'Mistral']['text'].iloc[0][:500])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT LENGTH STATISTICS:\")\n",
    "print(\"=\"*80)\n",
    "eval_df['text_length'] = eval_df['text'].str.len()\n",
    "print(eval_df.groupby('source')['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70b481",
   "metadata": {},
   "source": [
    "## Task 2: Load the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "ADAPTER_PATH = \"./lora_finetuned_model\"  # Update this path\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"LoRA adapter path: {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1200f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "\n",
    "# Merge adapter weights for faster inference\n",
    "print(\"Merging adapter weights...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba683363",
   "metadata": {},
   "source": [
    "## Task 3: Run Inference on Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_batch(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Run inference on a list of texts\n",
    "    Returns predictions (0=Human, 1=AI) and probabilities\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Running inference\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Store results\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions), np.array(probabilities)\n",
    "\n",
    "print(\"Inference function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76897ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all evaluation texts\n",
    "print(f\"Running inference on {len(eval_df)} texts...\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"This may take several minutes...\\n\")\n",
    "\n",
    "predictions, probabilities = predict_batch(eval_df['text'].tolist(), batch_size=32)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "eval_df['predicted_label'] = predictions\n",
    "eval_df['prob_human'] = probabilities[:, 0]\n",
    "eval_df['prob_ai'] = probabilities[:, 1]\n",
    "\n",
    "print(\"\\nInference complete!\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4d918",
   "metadata": {},
   "source": [
    "## Task 4: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "y_true = eval_df['true_label'].values\n",
    "y_pred = eval_df['predicted_label'].values\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "print(f\"Total samples: {len(y_true)}\")\n",
    "print(f\"Correct predictions: {(y_true == y_pred).sum()}\")\n",
    "print(f\"Incorrect predictions: {(y_true != y_pred).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Human (0)':<15} {precision[0]:<12.4f} {recall[0]:<12.4f} {f1[0]:<12.4f} {support[0]:<10}\")\n",
    "print(f\"{'AI/Mistral (1)':<15} {precision[1]:<12.4f} {recall[1]:<12.4f} {f1[1]:<12.4f} {support[1]:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(\"              Predicted\")\n",
    "print(\"              Human    AI/Mistral\")\n",
    "print(f\"Actual Human    {cm[0,0]:<8} {cm[0,1]:<8}\")\n",
    "print(f\"       AI       {cm[1,0]:<8} {cm[1,1]:<8}\")\n",
    "\n",
    "# Calculate specific error rates\n",
    "human_correct = cm[0, 0]\n",
    "human_total = support[0]\n",
    "human_accuracy = human_correct / human_total\n",
    "\n",
    "ai_correct = cm[1, 1]\n",
    "ai_total = support[1]\n",
    "ai_accuracy = ai_correct / ai_total\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASS-SPECIFIC ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Human texts correctly classified: {human_correct}/{human_total} ({human_accuracy*100:.2f}%)\")\n",
    "print(f\"AI texts correctly classified: {ai_correct}/{ai_total} ({ai_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nHuman texts misclassified as AI: {cm[0,1]} ({cm[0,1]/human_total*100:.2f}%)\")\n",
    "print(f\"AI texts misclassified as Human: {cm[1,0]} ({cm[1,0]/ai_total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7596d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_true, y_pred, target_names=['Human', 'AI/Mistral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded51f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "print(\"=\"*80)\n",
    "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall confidence statistics\n",
    "eval_df['confidence'] = eval_df[['prob_human', 'prob_ai']].max(axis=1)\n",
    "\n",
    "print(\"\\nOverall confidence statistics:\")\n",
    "print(eval_df['confidence'].describe())\n",
    "\n",
    "print(\"\\n\\nConfidence by prediction correctness:\")\n",
    "eval_df['correct'] = eval_df['true_label'] == eval_df['predicted_label']\n",
    "print(\"\\nCorrect predictions:\")\n",
    "print(eval_df[eval_df['correct']]['confidence'].describe())\n",
    "print(\"\\nIncorrect predictions:\")\n",
    "print(eval_df[~eval_df['correct']]['confidence'].describe())\n",
    "\n",
    "print(\"\\n\\nConfidence by true class:\")\n",
    "for source in ['Human', 'Mistral']:\n",
    "    print(f\"\\n{source} texts:\")\n",
    "    subset = eval_df[eval_df['source'] == source]\n",
    "    print(f\"  Mean confidence: {subset['confidence'].mean():.4f}\")\n",
    "    print(f\"  Median confidence: {subset['confidence'].median():.4f}\")\n",
    "    print(f\"  Min confidence: {subset['confidence'].min():.4f}\")\n",
    "    print(f\"  Max confidence: {subset['confidence'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine misclassified examples\n",
    "print(\"=\"*80)\n",
    "print(\"MISCLASSIFIED EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "misclassified = eval_df[~eval_df['correct']].copy()\n",
    "print(f\"\\nTotal misclassified: {len(misclassified)}\")\n",
    "\n",
    "# Show a few examples of each type of error\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"HUMAN TEXTS MISCLASSIFIED AS AI (False Positives)\")\n",
    "print(\"-\"*80)\n",
    "human_fp = misclassified[misclassified['source'] == 'Human'].head(3)\n",
    "for idx, row in human_fp.iterrows():\n",
    "    print(f\"\\nExample (confidence: {row['prob_ai']:.4f}):\")\n",
    "    print(row['text'][:300] + \"...\" if len(row['text']) > 300 else row['text'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"AI TEXTS MISCLASSIFIED AS HUMAN (False Negatives)\")\n",
    "print(\"-\"*80)\n",
    "ai_fn = misclassified[misclassified['source'] == 'Mistral'].head(3)\n",
    "for idx, row in ai_fn.iterrows():\n",
    "    print(f\"\\nExample (confidence: {row['prob_human']:.4f}):\")\n",
    "    print(row['text'][:300] + \"...\" if len(row['text']) > 300 else row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c24aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall acc:0.6925554999165414\n",
      "precision: [0.6200664727877025, 0.995995995995996]\n",
      "recall: [0.3891115564462258, 0.995995995995996]\n",
      "f1_score: [0.47816148060555236, 0.995995995995996]\n",
      "confusion_mtx: [[1165, 1829], [12, 2985]]\n",
      "human accuracy: 0.3891115564462258\n",
      "ai_accuracy: 0.995995995995996\n",
      "overall_confidence_stats: {'count': 5991.0, 'mean': 0.9426171875, 'std': 0.09650634765625, 'min': 0.5, '25%': 0.926015625, '50%': 0.986375, '75%': 0.998046875, 'max': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./big_data_res.pkl\",\"rb\") as file:\n",
    "    res = pickle.load(file)\n",
    "\n",
    "print(f\"overall acc:{res['overall_accuracy']}\")\n",
    "print(f\"precision: {res['precision']}\")\n",
    "print(f\"recall: {res['recall']}\")\n",
    "print(f\"f1_score: {res['f1_score']}\")\n",
    "print(f\"confusion_mtx: {res['confusion_matrix']}\")\n",
    "print(f\"human accuracy: {res['human_accuracy']}\")\n",
    "print(f\"ai_accuracy: {res['ai_accuracy']}\")\n",
    "print(f\"overall_confidence_stats: {res['overall_confidence_stats']}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
